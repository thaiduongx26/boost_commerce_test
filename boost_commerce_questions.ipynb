{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "554ac5d8-15c8-4915-8bfc-a2243b1e737e",
   "metadata": {},
   "source": [
    "# PROBLEM 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490b30d0-1c66-40db-96d2-982e2f5f7d00",
   "metadata": {},
   "source": [
    "## Part a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4e54a4-1699-4a10-bad8-13f9850c6052",
   "metadata": {},
   "source": [
    "Basically, we choose important features to recommend an item to users based on their purchase, a kind of content-based system. In my opinion, all features that help provide more information are important. In the current trend, researchers usually extend the size and the architecture to help models can deeply understand the connection between the features. But for this case, I will derive the important features below:\n",
    "- from user: age, sex, nationality\n",
    "- Product type\n",
    "- Title + Description: Can help to recommend \u001d",
    "items with similar or better functions (retrieved from text), sometimes it doesn't have the same product type.\n",
    "- Image: to recommend similar or better-looking items.\n",
    "- purchased timestamp may help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1744242-a822-4bda-86e7-a7af01481d10",
   "metadata": {},
   "source": [
    "## Part b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76690ea8-1325-4226-98d1-4bda5cbc75ec",
   "metadata": {},
   "source": [
    "In this case, we have the information of items that the user is interested in. So we gonna recommend new items based on items similarity using: \n",
    "- Item title + description: Tfidf, BM25, deep learning context meaning (BERT, RoBERTa, ...)\n",
    "- Image: CNN architecture\n",
    "\n",
    "<br>Apply reranking to get better embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29feae5a-b565-4f10-8dea-80d4f096c98b",
   "metadata": {},
   "source": [
    "# PROBLEM 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66f31d3-ed49-46fa-9bd5-a8c98a7263f5",
   "metadata": {},
   "source": [
    "## Part a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06511a09-b122-4ca5-9b82-5680f853987e",
   "metadata": {},
   "source": [
    "The work of Achlioptas only focuses on handling the cases of s = 1 and s = 3. With s = 3, only $1/3$ of the data needs to be processed and the memory can be reduced. From that, Very Sparse Random Projections extend the problem and make it more general. They tried to prove that one can use other larger factors like $\\sqrt{D}$ and still keep the performance. This means it only needs to process $\\frac{1}{\\sqrt{D}}$ of the data with little loss in accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b42697-d1c8-4665-88ff-34ec39cfb06c",
   "metadata": {},
   "source": [
    "Pros and cons of using big values of parameter s in very sparse random projection methods:\n",
    "- Pros: can achieve an s-fold speedup, save a lot of memory.\n",
    "- Cons: loss of the information of features when having too many zero-elements in projection matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e5ef2e-6de9-4829-99dd-fc757ee07242",
   "metadata": {},
   "source": [
    "## Part b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f1052d-af89-475a-a58d-06ab16f8ab0c",
   "metadata": {},
   "source": [
    "Because the requirement is quite matched with the idea of 'very sparse random projections'. We have a large dimension of features, we want to project it to a lower dimension. And I think just a small subset of sites was viewed by user, so maybe the features vector contains a lot of zero, so basically, we have a sparse matrix as the data. It can help to speed up the multiplication between 2 sparse matrixes using some libs, and it also reduces concerns about losing information when used with larger factors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f64b094-475e-4c7b-bf4f-b9c601501df6",
   "metadata": {},
   "source": [
    "The parameters I want to use: |U|x|S|, d, A(u,s), factor"
   ]
  },
  {
   "cell_type": "raw",
   "id": "60105813-542a-49d9-a0a9-8c8dbe99d43a",
   "metadata": {},
   "source": [
    "def transform(df01):\n",
    "    df01 = df01.sort_values(by=['site id'])\n",
    "    user_groups = df01.groupby('user id').indices\n",
    "    list_site_id = df01.select(\"site id\").distinct()\n",
    "    feature_matrix = build_features_matrix(df01, user_groups, list_site_id) # build |U|x|S| , S = len(list_site_id)\n",
    "    projector_matrix = make_random_sparse_matrix(len(list_site_id), d, factor) # target |U|x|d|\n",
    "    output_features = multiplication(feature_matrix, projector_matrix) # output |U|x|20|\n",
    "    user_columns = df01.select(\"user id\").distinct()\n",
    "    output = concat(user_columns, output_features)\n",
    "    df02 = pd.DataFrame(output, columns=['user id'] + [\"ft_\"+str(i).zfill(2) for i in range(1, 21)])\n",
    "    return df02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0793b6-b786-4fb2-aa6a-728f2f145060",
   "metadata": {},
   "source": [
    "# PROBLEM 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53f6092-3ae4-4e72-b39b-3261d15c04f2",
   "metadata": {},
   "source": [
    "## part a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f555248f-58ec-4027-9cb9-be40962d7f3a",
   "metadata": {},
   "source": [
    "The fundamental differences between mean and median:\n",
    "- mean was used to describe a <b>central tendency</b> by adding all list's values and then dividing by the length of list. It can say what value point is closest with <b>all</b> elements of the list.\n",
    "- median was used to find a <b>central value</b> in a sorted list.\n",
    "\n",
    "About housing prices in District 4, Ho Chi Minh City. I checked the prices in batdongsan.com and by my real estate knowledge, I'm quite sure that the mean would be higher than the median. \n",
    "<br>Because mean will be affected by extreme observations like too great values or too small values. And in real estate, we will see some so high prices like a penthouse or large plot of land, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d800b43-3f32-4d2d-9929-d30f66bd97ba",
   "metadata": {},
   "source": [
    "## part b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2442cb6e-6de3-4c80-a2bf-75bd2380e30d",
   "metadata": {},
   "source": [
    "We can use quickselect. The algorithm was used to find k-th smallest/largest element with O(n) in average case but O(n^2) in worst case. Can use the algo from https://en.wikipedia.org/wiki/Median_of_medians to get a better O(n) in worst case. But in practice, a lot of documents describe that it's not good on average."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b7c99385-9fa2-4512-a6b9-e3567da41b2c",
   "metadata": {},
   "source": [
    "def partition(arr, left, right) :\n",
    "    lst = arr[right]\n",
    "    i = left\n",
    "    for j in range(left, right):\n",
    "        if (arr[j] < lst) :\n",
    "            swap(arr[i], arr[j])\n",
    "            i += 1\n",
    "    swap(arr[i], arr[right])\n",
    "    return i\n",
    "\n",
    "def quickSelect(arr, left, right, k):\n",
    "    if left < right:\n",
    "        partitionIndex = partition(arr, left, right);\n",
    "        \n",
    "        If partition index = k, then we found the median of odd number element in arr[]:\n",
    "            second_median_element = arr[partitionIndex];\n",
    "                 \n",
    "        If partition index = k - 1, then we get first_median_element & second_median_element as middle element of arr[]:\n",
    "            first_median_element = arr[partitionIndex];\n",
    "                 \n",
    "        If partitionIndex >= k then find the index in first half of the arr[]:\n",
    "            quickSelect(arr, l, partitionIndex - 1, k);\n",
    "             \n",
    "        If partitionIndex <= k then find the index in second half of the arr[]:\n",
    "            quickSelect(arr, partitionIndex + 1, r, k);\n",
    "        \n",
    "    return when found both first_median_element & second_median_element\n",
    "\n",
    "def median(arr):\n",
    "    quickSelect(arr, 0, len(arr) - 1, k=len(arr) // 2)\n",
    "    if len(arr) is odd:\n",
    "        return second_median_element\n",
    "    return (first_median_element + second_median_element) / 2   # if len(arr) is even"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b3c94a-68c4-49bc-8e38-6709feb22538",
   "metadata": {},
   "source": [
    "# PROBLEM 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb5cf4b-3bcd-461a-a8db-1ce6612fb595",
   "metadata": {},
   "source": [
    "## part a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e32058-fd47-4c7f-bc53-a42e3699a066",
   "metadata": {},
   "source": [
    "It seem they used sort-merge join for the left join. So the computational complexity of sort is O(n log n), index lookup is O(1), so O(n) for n elements. So in this case, we got O(n log n)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc92922-c142-4dff-9cac-d26f4d2fc919",
   "metadata": {},
   "source": [
    "## part b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feeaab8-a56f-4584-9a11-55fcb178d851",
   "metadata": {},
   "source": [
    "Hash join seems suitable for this case. The computational complexity gonna be O(n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4c502a69-1729-4e84-9bbf-c4d47ff353b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import nan as Nan\n",
    "\n",
    "df1 = pd.DataFrame({'usr_id': ['K0', 'K1', 'K2', 'K3', 'K4', 'K5'],\n",
    "                   'feature_1': [3, 5, 1, 6, 6, 8],\n",
    "                   'feature_2': [1, 9, 2, 6, 4, 10],\n",
    "                   'feature_3': [0, 9, 13, 5, 2, 12]})\n",
    "df2 = pd.DataFrame({'usr_id': ['K2', 'K1', 'K0', 'K3', 'K4', 'K6'],\n",
    "                     'feature_4': [123, 55, 1, 11, 4, 1],\n",
    "                     'feature_5': [1, 12, 4, 6, 89, 100]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f9aec18c-6501-4ff2-b6d4-8ecb4df1363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_left_join(df1, df2, on='usr_id', lsuffix='l_', rsuffix='r_'):\n",
    "    \n",
    "    def get_key_dict(df, on='usr_id'):\n",
    "        return {value: index for index, value in enumerate(df[on].values)}\n",
    "    \n",
    "    df2_key_dict = get_key_dict(df2, on='usr_id') # O(n)\n",
    "    # process keys\n",
    "    df1_keys = df1.keys()\n",
    "    df2_keys = df2.keys()\n",
    "    new_keys = [on]\n",
    "    for key in df1_keys:\n",
    "        if key == on:\n",
    "            continue\n",
    "        new_key = lsuffix + key if key != on and key in df2_keys else key\n",
    "        new_keys.append(new_key)\n",
    "        \n",
    "    for key in df2_keys:\n",
    "        if key == on:\n",
    "            continue\n",
    "        new_key = rsuffix + key if key != on and key in df1_keys else key\n",
    "        new_keys.append(new_key)\n",
    "    \n",
    "    # contruct new join df\n",
    "    rows_value = []\n",
    "    # loop every row in df1 => O(n)\n",
    "    for idx, value in enumerate(df1[on].values):\n",
    "        row_value = [value]\n",
    "        row_value.extend([df1[key][idx] for key in df1_keys if key != on])\n",
    "        if value in df2_key_dict: # O(1)\n",
    "            row_value.extend([df2[key][idx] for key in df2_keys if key != on])\n",
    "        else:\n",
    "            row_value.extend([Nan for key in df2_keys if key != on])\n",
    "        rows_value.append(row_value)\n",
    "    return pd.DataFrame(rows_value, columns=new_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "83d635ed-4530-4c12-8d16-22527595edcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>usr_id</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>K0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>K1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>55.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>K2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>K3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>K4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>K5</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  usr_id  feature_1  feature_2  feature_3  feature_4  feature_5\n",
       "0     K0          3          1          0      123.0        1.0\n",
       "1     K1          5          9          9       55.0       12.0\n",
       "2     K2          1          2         13        1.0        4.0\n",
       "3     K3          6          6          5       11.0        6.0\n",
       "4     K4          6          4          2        4.0       89.0\n",
       "5     K5          8         10         12        NaN        NaN"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_left_join(df1, df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a8e899-f2be-44b5-ba13-e50783e1ed08",
   "metadata": {},
   "source": [
    "# PROBLEM 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c1dd4d-125f-46c4-a24e-d1836ee35a8a",
   "metadata": {},
   "source": [
    "## Part a: Scenario [S01]: n < 1,000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a89c1b-5a46-4655-bf78-60ba895f8665",
   "metadata": {},
   "source": [
    "Because n is quite small so I think just my first idea is enough. Firstly, sort the all values by using app column.Then, use groupby to group the row by using usr_id column. We can handle first 2 steps with 2 lines of code. After that, make a new dataframe and transform to the target output.\n",
    "<br> sort: O(n log n)\n",
    "<br> groupby: O(n)\n",
    "<br> => O(n log n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f303c5-172b-4561-9650-6bf6b2964ec7",
   "metadata": {},
   "source": [
    "## Part b: Scenario [S02]: n > 10,000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ad745d-0cc2-4287-9987-568059a81a34",
   "metadata": {},
   "source": [
    "With n very large, let reduce the sort step. The role of the sort step is mapping frequencies with new column names. So we can replace it by using dict to map the app value to column names. \n",
    "<br>Build the dict: O(m) with m is the length of df01\n",
    "<br>Group by usr_id: O(m)\n",
    "<br>Map the value to new column: O(1) for each get from dict => O(n) for n apps\n",
    "<br>=> O(m + n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514c1269-8f73-49df-926d-3f9eb5a995a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
